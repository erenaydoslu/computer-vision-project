{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53d600fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BeitFeatureExtractor, BeitForImageClassification\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from dataset import SingleImageDataset, FullLocationDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc7ca435",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SingleImageDataset()\n",
    "train_set, val_set, test_set = random_split(dataset, [0.6, 0.2, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0338667",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=96, num_workers=4, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_set, batch_size=96, num_workers=4, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_set, batch_size=96, num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a491c68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\erena\\anaconda3\\envs\\Torch-DL\\Lib\\site-packages\\transformers\\models\\beit\\feature_extraction_beit.py:28: FutureWarning: The class BeitFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use BeitImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=2, bias=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor = BeitFeatureExtractor.from_pretrained('microsoft/beit-base-patch16-384')\n",
    "model = BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-384').to(\"cuda\")\n",
    "\n",
    "model.classifier = torch.nn.Linear(768, 2).to(\"cuda\")\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c44e0bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    # Radius of the Earth in kilometers\n",
    "    R = 6371.0\n",
    "\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1_rad = torch.deg2rad(lat1)\n",
    "    lon1_rad = torch.deg2rad(lon1)\n",
    "    lat2_rad = torch.deg2rad(lat2)\n",
    "    lon2_rad = torch.deg2rad(lon2)\n",
    "\n",
    "    # Differences in coordinates\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    dlon = lon2_rad - lon1_rad    \n",
    "\n",
    "    a = torch.sin(dlat / 2)**2 + torch.cos(lat1_rad) * torch.cos(lat2_rad) * torch.sin(dlon / 2)**2\n",
    "    c = 2 * torch.atan2(torch.sqrt(a), torch.sqrt(1 - a))\n",
    "\n",
    "    distance = R * c\n",
    "    return distance\n",
    "\n",
    "class HaversineLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HaversineLoss, self).__init__()\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        # Assuming outputs and targets are both of shape (batch_size, 2)\n",
    "        lat1, lon1 = outputs[:, 0], outputs[:, 1]\n",
    "        lat2, lon2 = targets[:, 0], targets[:, 1]\n",
    "        \n",
    "        distances = haversine_distance(lat1, lon1, lat2, lon2)\n",
    "        return distances.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d2339da",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = HaversineLoss()\n",
    "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eced5ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6272b9caa1b44ab6ab7067a94f1e31b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Train Loss: 5235.01 \t Val Loss: 5263.53\n",
      "Epoch: 2 \t Train Loss: 5250.18 \t Val Loss: 5239.48\n",
      "Epoch: 3 \t Train Loss: 5135.81 \t Val Loss: 5282.84\n",
      "Epoch: 4 \t Train Loss: 5220.97 \t Val Loss: 5204.14\n",
      "Epoch: 5 \t Train Loss: 5240.89 \t Val Loss: 5249.39\n",
      "Epoch: 6 \t Train Loss: 5224.96 \t Val Loss: 5348.62\n",
      "Epoch: 7 \t Train Loss: 5182.38 \t Val Loss: 5388.10\n",
      "Epoch: 8 \t Train Loss: 5315.13 \t Val Loss: 5253.56\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m         y_pred \u001b[38;5;241m=\u001b[39m model(features)\n\u001b[0;32m     34\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(y_pred\u001b[38;5;241m.\u001b[39mlogits, labels)\n\u001b[1;32m---> 35\u001b[0m         running_val_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     37\u001b[0m val_losses\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(running_val_loss))\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_train_loss[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_val_loss[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    running_train_loss = []\n",
    "    running_val_loss = []\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        labels = labels.cuda(non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        features = feature_extractor(images, return_tensors=\"pt\")\n",
    "        features = features['pixel_values'].cuda(non_blocking=True)\n",
    "\n",
    "        y_pred = model(features)\n",
    "\n",
    "        loss = criterion(y_pred.logits, labels)\n",
    "        loss.backward()\n",
    "        running_train_loss.append(loss.item())\n",
    "\n",
    "    train_losses.append(np.mean(running_train_loss))\n",
    "\n",
    "    for images, labels in val_loader:\n",
    "        with torch.no_grad():\n",
    "            labels = labels.cuda(non_blocking=True)\n",
    "\n",
    "            features = feature_extractor(images, return_tensors=\"pt\")\n",
    "            features = features['pixel_values'].cuda(non_blocking=True)\n",
    "\n",
    "            y_pred = model(features)\n",
    "\n",
    "            loss = criterion(y_pred.logits, labels)\n",
    "            running_val_loss.append(loss.item())\n",
    "\n",
    "    val_losses.append(np.mean(running_val_loss))\n",
    "\n",
    "    print(f\"Epoch: {epoch} \\t Train Loss: {running_train_loss[-1]:.2f} \\t Val Loss: {running_val_loss[-1]:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch-DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
